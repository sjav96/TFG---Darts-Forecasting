{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YeoFcfGa7E3-",
    "outputId": "0cebf484-c1df-4e26-cbef-4b4d62522d3c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.keras import backend as Kc\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import scipy as sp\n",
    "import pickle\n",
    "import math\n",
    "import pandas as pd\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "SMALL_SIZE = 15\n",
    "MEDIUM_SIZE = 18\n",
    "BIGGER_SIZE = 22\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20.0, 5.0)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(result, filename):\n",
    "    \n",
    "    with open(filename, \"wb\") as output:\n",
    "        pickle.dump(result, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_result(filename):\n",
    "    \n",
    "    with open(filename, \"rb\") as input:\n",
    "        return pickle.load(input)\n",
    "    \n",
    "def load_dataset(name='', index=''):\n",
    "    \n",
    "    dataframe = pd.read_csv(name)\n",
    "    dataframe.set_index('date', inplace=True)\n",
    "    #dataframe=dataframe[dataframe['Year'].between(2015, 2020)]\n",
    "    \n",
    "    print('Features:', [i for i in dataframe.columns])\n",
    "    print('Range: ', dataframe.index[0],\" ~ \",dataframe.index[-1])\n",
    "    return dataframe\n",
    "\n",
    "def input_output_splitter(series, historical_window:int, prediction_window:int, jump:bool=False, lstm:bool=False):\n",
    "\n",
    "    input_index = []\n",
    "    input_value = []\n",
    "    output_index = []\n",
    "    output_value = []\n",
    "    step = 1\n",
    "    number_predictions = len(series) - historical_window - prediction_window + 1\n",
    "    \n",
    "    if jump:\n",
    "        step = prediction_window\n",
    "        number_predictions = (len(series) - historical_window)//prediction_window\n",
    "        \n",
    "    for i in tqdm(range(number_predictions)):\n",
    "        input_index.append(series.index[i*step:i*step+historical_window])\n",
    "        \n",
    "        if lstm:\n",
    "            if series.ndim == 1: input_value.append(series.values[i*step:i*step+historical_window].astype('float32'))\n",
    "            else: input_value.append(series.values[i*step:i*step+historical_window, :].astype('float32'))\n",
    "        else:\n",
    "            if series.ndim == 1: input_value.append(series.values[i*step:i*step+historical_window].astype('float32'))\n",
    "            else: input_value.append(series.values[i*step:i*step+historical_window, :].astype('float32').ravel())\n",
    "\n",
    "        output_index.append(series.index[i*step+historical_window:i*step+historical_window+prediction_window][0])\n",
    "        \n",
    "        \n",
    "        if series.ndim == 1: output_value.append(series.values[i*step+historical_window:i*step+historical_window+prediction_window].astype('float32'))\n",
    "        else: output_value.append(series.values[i*step+historical_window:i*step+historical_window+prediction_window, 0].astype('float32'))\n",
    "\n",
    "    input_index = pd.DataFrame(input_index)\n",
    "    input_value = np.array(input_value)\n",
    "    \n",
    "    if lstm:\n",
    "        if series.ndim == 1: input_value = input_value[..., np.newaxis]\n",
    "    \n",
    "    output_index = pd.DataFrame(output_index, columns=[\"Date\"])\n",
    "    output_value = np.array(output_value)\n",
    "    \n",
    "    return np.array(input_value), input_index, np.array(output_value), output_index\n",
    "\n",
    "def dataset_splitter(input_data, output_data, data_range, indexes, lstm:bool=False):\n",
    "\n",
    "    indexes[\"Date\"] = pd.to_datetime(indexes[\"Date\"])\n",
    "    start = indexes[indexes[\"Date\"] >= data_range[0]].index[0]\n",
    "    stop = indexes[indexes[\"Date\"] <= data_range[1]].index[-1]\n",
    "    \n",
    "    x = input_data[start:stop+1, :, :] if lstm else input_data[start:stop+1, :]\n",
    "    y = output_data[start:stop+1, :]\n",
    "        \n",
    "    return x, y\n",
    "\n",
    "def dataset_scaling(dataset):\n",
    "       \n",
    "    index = dataset.index\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    df = scaler.fit_transform(dataset)\n",
    "    df = pd.DataFrame(df, columns=dataset.columns, index=index)\n",
    "    \n",
    "    return df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "text",
    "id": "PjLRLtTX7FQk"
   },
   "outputs": [],
   "source": [
    "def data():\n",
    "    name= 'procesado.csv'\n",
    "    df = pd.read_csv(name)\n",
    "    df.set_index('date', inplace=True)\n",
    "    #dataframe=dataframe[dataframe['Year'].between(2015, 2020)]\n",
    "    \n",
    "    pred=24\n",
    "    df[\"Feriados\"] = ((pd.to_datetime(df.index).month== 12) & (pd.to_datetime(df.index).day == 25)).astype(int)\n",
    "    df[\"Feriados\"] = ((pd.to_datetime(df.index).month == 12) & (pd.to_datetime(df.index).day == 24)).astype(int)\n",
    "    df[\"Feriados\"] = ((pd.to_datetime(df.index).month == 12) & (pd.to_datetime(df.index).day == 31)).astype(int)\n",
    "    df[\"Feriados\"] = ((pd.to_datetime(df.index).month == 1) & (pd.to_datetime(df.index).day == 1)).astype(int)\n",
    "    df[\"SIN_weekpast\"] = df[\"SIN\"].shift(periods=(pred*7))\n",
    "    df[\"temp_t1\"] = df[\"Temperatura Asuncion\"].shift(periods=-pred)\n",
    "    df[\"Year_t1\"] = df[\"Year\"].shift(periods=-pred)\n",
    "    df[\"Hour_t1\"] = df[\"Hour\"].shift(periods=-pred)\n",
    "    df[\"Month_t1\"] = df[\"Month\"].shift(periods=-pred)\n",
    "    df[\"Weekday_t1\"] = df[\"Weekday\"].shift(periods=-pred)\n",
    "    df[\"Feriados_t1\"] = df[\"Feriados\"].shift(periods=-pred)\n",
    "    df = df.dropna()\n",
    "    dataset=df[[ \"SIN\",\"SIN_weekpast\",\"Temperatura Asuncion\", \"temp_t1\", \"Year_t1\", \"Weekday_t1\", \"Month_t1\",\"Feriados_t1\"]]\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def create_model(dataset):\n",
    "    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "    def input_output_splitter(series, historical_window:int, prediction_window:int, jump:bool=False, lstm:bool=False):\n",
    "\n",
    "        input_index = []\n",
    "        input_value = []\n",
    "        output_index = []\n",
    "        output_value = []\n",
    "        step = 1\n",
    "        number_predictions = len(series) - historical_window - prediction_window + 1\n",
    "\n",
    "        if jump:\n",
    "            step = prediction_window\n",
    "            number_predictions = (len(series) - historical_window)//prediction_window\n",
    "\n",
    "        for i in tqdm(range(number_predictions)):\n",
    "            input_index.append(series.index[i*step:i*step+historical_window])\n",
    "\n",
    "            if lstm:\n",
    "                if series.ndim == 1: input_value.append(series.values[i*step:i*step+historical_window].astype('float32'))\n",
    "                else: input_value.append(series.values[i*step:i*step+historical_window, :].astype('float32'))\n",
    "            else:\n",
    "                if series.ndim == 1: input_value.append(series.values[i*step:i*step+historical_window].astype('float32'))\n",
    "                else: input_value.append(series.values[i*step:i*step+historical_window, :].astype('float32').ravel())\n",
    "\n",
    "            output_index.append(series.index[i*step+historical_window:i*step+historical_window+prediction_window][0])\n",
    "\n",
    "\n",
    "            if series.ndim == 1: output_value.append(series.values[i*step+historical_window:i*step+historical_window+prediction_window].astype('float32'))\n",
    "            else: output_value.append(series.values[i*step+historical_window:i*step+historical_window+prediction_window, 0].astype('float32'))\n",
    "\n",
    "        input_index = pd.DataFrame(input_index)\n",
    "        input_value = np.array(input_value)\n",
    "\n",
    "        if lstm:\n",
    "            if series.ndim == 1: input_value = input_value[..., np.newaxis]\n",
    "\n",
    "        output_index = pd.DataFrame(output_index, columns=[\"Date\"])\n",
    "        output_value = np.array(output_value)\n",
    "\n",
    "        return np.array(input_value), input_index, np.array(output_value), output_index\n",
    "\n",
    "    def dataset_splitter(input_data, output_data, data_range, indexes, lstm:bool=False):\n",
    "\n",
    "        indexes[\"Date\"] = pd.to_datetime(indexes[\"Date\"])\n",
    "        start = indexes[indexes[\"Date\"] >= data_range[0]].index[0]\n",
    "        stop = indexes[indexes[\"Date\"] <= data_range[1]].index[-1]\n",
    "\n",
    "        x = input_data[start:stop+1, :, :] if lstm else input_data[start:stop+1, :]\n",
    "        y = output_data[start:stop+1, :]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def dataset_scaling(dataset):\n",
    "\n",
    "        index = dataset.index\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "        df = scaler.fit_transform(dataset)\n",
    "        df = pd.DataFrame(df, columns=dataset.columns, index=index)\n",
    "\n",
    "        return df, scaler\n",
    "    eval_look_back = {{choice([i for i in range(24,168)])}}\n",
    "    eval_num_units = {{choice([i for i in range(12,168)])}}\n",
    "    dataset_norm, _ = dataset_scaling(dataset)\n",
    "    _, scalerSin = dataset_scaling(dataset.filter([\"SIN\"],axis=1))\n",
    "\n",
    "    X, x_ind, Y, y_ind = input_output_splitter(dataset_norm, historical_window=eval_look_back, \n",
    "                                                   prediction_window=pred, jump=True, lstm=True)\n",
    "    train_range = ('2015-01-01', '2018-12-31')\n",
    "    val_range =('2019-01-01', '2019-12-31')\n",
    "    X_train, Y_train = dataset_splitter(X, Y, train_range, y_ind)\n",
    "    X_val, Y_val = dataset_splitter(X, Y, val_range, y_ind)\n",
    "    print(f'\\nVentana historica: {eval_look_back}\\n'\n",
    "            f'Numero de unidades LSTM: {eval_num_units}')\n",
    "    n_features = len(dataset_norm.columns)\n",
    "  \n",
    "    model = Sequential([CuDNNLSTM(eval_num_units, input_shape=(eval_look_back, n_features)),\n",
    "                        Dense(eval_num_units, activation='relu'),\n",
    "                        Dense(pred, activation='linear')])\n",
    "\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"Adam\")\n",
    "\n",
    "    start = time.time()\n",
    "    hist = model.fit(X_train, Y_train, epochs=100, shuffle=True, batch_size=64,\n",
    "                         validation_data=(X_val, Y_val), verbose=0)\n",
    "    end = time.time()\n",
    "    print('Time training:', end-start)\n",
    "\n",
    "    model.summary()\n",
    "    Y_prediction_norm = model.predict(X_val)\n",
    "    Y_prediction = scalerSin.inverse_transform(Y_prediction_norm)\n",
    "    Y_valInv = scalerSin.inverse_transform(Y_val)\n",
    "    rmse=np.sqrt(mean_squared_error(np.array(Y_valInv), Y_prediction)) \n",
    "    \n",
    "    print('RMSE:',rmse)\n",
    "    print('R2:',r2_score(np.array(Y_valInv), Y_prediction))\n",
    "    print('MAPE:',mean_absolute_percentage_error(np.array(Y_valInv), Y_prediction))\n",
    "    MAPE=np.abs((Y_valInv - Y_prediction) /Y_valInv)\n",
    "    print('10thMAPE:',np.percentile(MAPE,10))\n",
    "    print('50thMAPE:',np.percentile(MAPE,50))\n",
    "    print('75thMAPE:',np.percentile(MAPE,75))\n",
    "    print('90thMAPE:',np.percentile(MAPE,90))\n",
    "    print('MaxMAPE:',np.max(MAPE))\n",
    "    validation_acc = rmse \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    \n",
    "    return {'loss': validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vsWQf37gHfBP",
    "outputId": "6da9409e-76b6-4afb-f31f-ae3bbb75a609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.dates as mdates\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.ticker as ticker\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import seaborn as sns\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tqdm import tqdm\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.compat.v1.keras import backend as Kc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import random as rn\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import scipy as sp\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pickle\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import math\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import time\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from timeit import default_timer as timer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import MinMaxScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'eval_look_back': hp.choice('eval_look_back', [i for i in range(24,168)]),\n",
      "        'eval_num_units': hp.choice('eval_num_units', [i for i in range(12,168)]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: name= 'procesado.csv'\n",
      "   3: df = pd.read_csv(name)\n",
      "   4: df.set_index('date', inplace=True)\n",
      "   5: #dataframe=dataframe[dataframe['Year'].between(2015, 2020)]\n",
      "   6: \n",
      "   7: pred=24\n",
      "   8: df[\"Feriados\"] = ((pd.to_datetime(df.index).month== 12) & (pd.to_datetime(df.index).day == 25)).astype(int)\n",
      "   9: df[\"Feriados\"] = ((pd.to_datetime(df.index).month == 12) & (pd.to_datetime(df.index).day == 24)).astype(int)\n",
      "  10: df[\"Feriados\"] = ((pd.to_datetime(df.index).month == 12) & (pd.to_datetime(df.index).day == 31)).astype(int)\n",
      "  11: df[\"Feriados\"] = ((pd.to_datetime(df.index).month == 1) & (pd.to_datetime(df.index).day == 1)).astype(int)\n",
      "  12: df[\"SIN_weekpast\"] = df[\"SIN\"].shift(periods=(pred*7))\n",
      "  13: df[\"temp_t1\"] = df[\"Temperatura Asuncion\"].shift(periods=-pred)\n",
      "  14: df[\"Year_t1\"] = df[\"Year\"].shift(periods=-pred)\n",
      "  15: df[\"Hour_t1\"] = df[\"Hour\"].shift(periods=-pred)\n",
      "  16: df[\"Month_t1\"] = df[\"Month\"].shift(periods=-pred)\n",
      "  17: df[\"Weekday_t1\"] = df[\"Weekday\"].shift(periods=-pred)\n",
      "  18: df[\"Feriados_t1\"] = df[\"Feriados\"].shift(periods=-pred)\n",
      "  19: df = df.dropna()\n",
      "  20: dataset=df[[ \"SIN\",\"SIN_weekpast\",\"Temperatura Asuncion\", \"temp_t1\", \"Year_t1\", \"Weekday_t1\", \"Month_t1\",\"Feriados_t1\"]]\n",
      "  21: \n",
      "  22: \n",
      "  23: \n",
      "  24: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
      "   4:     tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
      "   5: \n",
      "   6:     def input_output_splitter(series, historical_window:int, prediction_window:int, jump:bool=False, lstm:bool=False):\n",
      "   7: \n",
      "   8:         input_index = []\n",
      "   9:         input_value = []\n",
      "  10:         output_index = []\n",
      "  11:         output_value = []\n",
      "  12:         step = 1\n",
      "  13:         number_predictions = len(series) - historical_window - prediction_window + 1\n",
      "  14: \n",
      "  15:         if jump:\n",
      "  16:             step = prediction_window\n",
      "  17:             number_predictions = (len(series) - historical_window)//prediction_window\n",
      "  18: \n",
      "  19:         for i in tqdm(range(number_predictions)):\n",
      "  20:             input_index.append(series.index[i*step:i*step+historical_window])\n",
      "  21: \n",
      "  22:             if lstm:\n",
      "  23:                 if series.ndim == 1: input_value.append(series.values[i*step:i*step+historical_window].astype('float32'))\n",
      "  24:                 else: input_value.append(series.values[i*step:i*step+historical_window, :].astype('float32'))\n",
      "  25:             else:\n",
      "  26:                 if series.ndim == 1: input_value.append(series.values[i*step:i*step+historical_window].astype('float32'))\n",
      "  27:                 else: input_value.append(series.values[i*step:i*step+historical_window, :].astype('float32').ravel())\n",
      "  28: \n",
      "  29:             output_index.append(series.index[i*step+historical_window:i*step+historical_window+prediction_window][0])\n",
      "  30: \n",
      "  31: \n",
      "  32:             if series.ndim == 1: output_value.append(series.values[i*step+historical_window:i*step+historical_window+prediction_window].astype('float32'))\n",
      "  33:             else: output_value.append(series.values[i*step+historical_window:i*step+historical_window+prediction_window, 0].astype('float32'))\n",
      "  34: \n",
      "  35:         input_index = pd.DataFrame(input_index)\n",
      "  36:         input_value = np.array(input_value)\n",
      "  37: \n",
      "  38:         if lstm:\n",
      "  39:             if series.ndim == 1: input_value = input_value[..., np.newaxis]\n",
      "  40: \n",
      "  41:         output_index = pd.DataFrame(output_index, columns=[\"Date\"])\n",
      "  42:         output_value = np.array(output_value)\n",
      "  43: \n",
      "  44:         return np.array(input_value), input_index, np.array(output_value), output_index\n",
      "  45: \n",
      "  46:     def dataset_splitter(input_data, output_data, data_range, indexes, lstm:bool=False):\n",
      "  47: \n",
      "  48:         indexes[\"Date\"] = pd.to_datetime(indexes[\"Date\"])\n",
      "  49:         start = indexes[indexes[\"Date\"] >= data_range[0]].index[0]\n",
      "  50:         stop = indexes[indexes[\"Date\"] <= data_range[1]].index[-1]\n",
      "  51: \n",
      "  52:         x = input_data[start:stop+1, :, :] if lstm else input_data[start:stop+1, :]\n",
      "  53:         y = output_data[start:stop+1, :]\n",
      "  54: \n",
      "  55:         return x, y\n",
      "  56: \n",
      "  57:     def dataset_scaling(dataset):\n",
      "  58: \n",
      "  59:         index = dataset.index\n",
      "  60:         scaler = MinMaxScaler(feature_range=(0, 1))\n",
      "  61: \n",
      "  62:         df = scaler.fit_transform(dataset)\n",
      "  63:         df = pd.DataFrame(df, columns=dataset.columns, index=index)\n",
      "  64: \n",
      "  65:         return df, scaler\n",
      "  66:     eval_look_back = space['eval_look_back']\n",
      "  67:     eval_num_units = space['eval_num_units']\n",
      "  68:     dataset_norm, _ = dataset_scaling(dataset)\n",
      "  69:     _, scalerSin = dataset_scaling(dataset.filter([\"SIN\"],axis=1))\n",
      "  70: \n",
      "  71:     X, x_ind, Y, y_ind = input_output_splitter(dataset_norm, historical_window=eval_look_back, \n",
      "  72:                                                    prediction_window=pred, jump=True, lstm=True)\n",
      "  73:     train_range = ('2015-01-01', '2018-12-31')\n",
      "  74:     val_range =('2019-01-01', '2019-12-31')\n",
      "  75:     X_train, Y_train = dataset_splitter(X, Y, train_range, y_ind)\n",
      "  76:     X_val, Y_val = dataset_splitter(X, Y, val_range, y_ind)\n",
      "  77:     print(f'\\nVentana historica: {eval_look_back}\\n'\n",
      "  78:             f'Numero de unidades LSTM: {eval_num_units}')\n",
      "  79:     n_features = len(dataset_norm.columns)\n",
      "  80:   \n",
      "  81:     model = Sequential([CuDNNLSTM(eval_num_units, input_shape=(eval_look_back, n_features)),\n",
      "  82:                         Dense(eval_num_units, activation='relu'),\n",
      "  83:                         Dense(pred, activation='linear')])\n",
      "  84: \n",
      "  85:     model.compile(loss=\"mean_squared_error\", optimizer=\"Adam\")\n",
      "  86: \n",
      "  87:     start = time.time()\n",
      "  88:     hist = model.fit(X_train, Y_train, epochs=100, shuffle=True, batch_size=64,\n",
      "  89:                          validation_data=(X_val, Y_val), verbose=0)\n",
      "  90:     end = time.time()\n",
      "  91:     print('Time training:', end-start)\n",
      "  92: \n",
      "  93:     model.summary()\n",
      "  94:     Y_prediction_norm = model.predict(X_val)\n",
      "  95:     Y_prediction = scalerSin.inverse_transform(Y_prediction_norm)\n",
      "  96:     Y_valInv = scalerSin.inverse_transform(Y_val)\n",
      "  97:     rmse=np.sqrt(mean_squared_error(np.array(Y_valInv), Y_prediction)) \n",
      "  98:     \n",
      "  99:     print('RMSE:',rmse)\n",
      " 100:     print('R2:',r2_score(np.array(Y_valInv), Y_prediction))\n",
      " 101:     print('MAPE:',mean_absolute_percentage_error(np.array(Y_valInv), Y_prediction))\n",
      " 102:     MAPE=np.abs((Y_valInv - Y_prediction) /Y_valInv)\n",
      " 103:     print('10thMAPE:',np.percentile(MAPE,10))\n",
      " 104:     print('50thMAPE:',np.percentile(MAPE,50))\n",
      " 105:     print('75thMAPE:',np.percentile(MAPE,75))\n",
      " 106:     print('90thMAPE:',np.percentile(MAPE,90))\n",
      " 107:     print('MaxMAPE:',np.max(MAPE))\n",
      " 108:     validation_acc = rmse \n",
      " 109:     print('Best validation acc of epoch:', validation_acc)\n",
      " 110:     \n",
      " 111:     return {'loss': validation_acc, 'status': STATUS_OK, 'model': model}\n",
      " 112: \n",
      "  0%|          | 0/70 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4733 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      " 32%|###1      | 1492/4733 [00:00<00:00, 14912.01it/s]\n",
      "\u001b[A\n",
      " 63%|######3   | 2984/4733 [00:00<00:00, 14558.59it/s]\n",
      "\u001b[A\n",
      " 94%|#########3| 4448/4733 [00:00<00:00, 14592.58it/s]\n",
      "\u001b[A\n",
      "100%|##########| 4733/4733 [00:00<00:00, 14461.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "Ventana historica: 161\n",
      "Numero de unidades LSTM: 143\n",
      "  0%|          | 0/70 [00:00<?, ?trial/s, best loss=?]"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=70,\n",
    "                                          trials=Trials(),notebook_name='bayes_hyperSeach_final')\n",
    "\n",
    "elapsed = timer()-start\n",
    "print(\"TOOk  \",elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "7IO8kbBqCpdb",
    "outputId": "fa26d0de-d987-478d-cf3f-29b575b26154"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_62 (CuDNNLSTM)    (None, 151)               97244     \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 151)               22952     \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 24)                3648      \n",
      "=================================================================\n",
      "Total params: 123,844\n",
      "Trainable params: 123,844\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "15QzoLYc4Vbe",
    "outputId": "ca42414a-b3e7-46a5-ac1d-d80be22a3e5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_look_back': 118, 'eval_num_units': 139}\n"
     ]
    }
   ],
   "source": [
    "print(best_run)\n",
    "#best_epochs = 20\n",
    "# add 1 to each params except unifor i.e. lr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rc0PzHe2oxC_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8230.83627946116"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "imdb_hyperSeach.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
